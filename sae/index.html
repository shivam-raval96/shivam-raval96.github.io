<!doctype html>
<meta charset="utf-8">
<head>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js" ></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../distill.template.v1.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <link rel="stylesheet" href="styles.css">
  <script src="./makeplots.js"></script>

</head>

<script type="text/front-matter">
  title: "Toy SAE"
  description: "An exploration of sparse autoencoders trained on synthetic data"
  authors:
  - TBD: https://shivam-raval96.github.io/
  affiliations:
  - Harvard University
</script>


<dt-article >
  <div class="l-page">
  <h1>An exploration of sparse autoencoders trained on synthetic data</h1>
    <h2>[Under construction]</h2>
    <dt-byline></dt-byline>
    <p>The Anthropic interpretability team recently showed Sparse Autoencoders (SAEs) can extract interpretable 
      concepts from Claude 3 Sonnet<dt-cite key="templeton2024scaling"></dt-cite>. In the following weeks, 
      OpenAI <dt-cite key="gao2024scalingsae"></dt-cite> and  DeepMind <dt-cite key="templeton2024scaling"></dt-cite> showcased similar results using their own variant autoencoders, Gated SAEs (DeepMind) and K-Sparse AEs (OpenAI)
      
      
      We would like to understand how these models work, what sort of features they can learn, and replicate some of the phenomena observed in large models ourselves. 
      For our experiments we will train small models on synthetic data, but in a similar fashion to these works, and mechanistically study the trained models.
      Two phenomena observed while training the model are quite interesting: <a href="https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes">"Shrinkage"</a> <dt-cite key="Wright2024shrinkage"></dt-cite> and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-feature-splitting">"Splitting"</a> <dt-cite key="bricken2023monosemanticity"></dt-cite>.
    </p>

    

    <h2>Sparse Autoencoders as feature extractors</h2>
    Autoencoders (AE) generally consist of an encoder and a decoder, 
    where the encoder maps the input to a latent space and the decoder reconstructs the input from this 
    latent space. One of the key objectives of an autoencoder is to capture the most important features 
    of the data necessary to reconstruct it. A sparse autoencoder (SAE) adds a sparsity penalty (L1 loss) to the loss function to learn sparse representations. For our purposes, we want to decompose an input \( x^j \in \mathbb{R}^d \) into a sparse, linear combination of feature directions:
    \[
    x^j = x^j_0 + \sum_{i=1}^M f_i(x^j) d_i,
    \]
    where \( d_i \in \mathbb{R}^M \) are \( M \gg d \) are the feature directions, and the sparse coefficients \( f_i(x^j) \geq 0 \) are
    the corresponding feature activations for the directions. Since it is possible to arbitrarily reduce the sparsity loss term without affecting reconstructions, we might have to constrain the norms of the columns of \( W_{\text{dec}} \)
    during training, as suggested by previous work from DeepMind<dt-cite key="rajamanoharan2024improving"></dt-cite>. Thus, this will be our model: for some input \(\mathbf{x} \in \mathbb{R}^d\), the encoder is:
    \[
    \mathbf{h} = \text{ReLU}(W_{\text{enc}} (x - b_{\text{dec}}) + b_{\text{enc}})
    \]
    The decoder reconstructs the input and can be represented as:
    \[
    \mathbf{\hat{x}} =  W_{\text{dec}} \mathbf{h} + b_{\text{dec}}
    \]

    The loss is the sum of the error in reconstruction of the input (typically the MSE) and a sparsity penalty on L1 penalty on the activations \(\mathbf{h} \in \mathbb{R}^M\)
    \[
  L = L_{\text{reconstruction}} + \lambda L_{\text{sparsity}} = \|\mathbf{x} - \mathbf{\hat{x}}\|^2_2 + \lambda \sum_{i=1}^M |h_i|
  \]  

  <h2>What causes feature shrinkage?</h2>
    <p><b>Case I: No \(L_1\) regularization</b> \[L = \sum \| \hat{x} - x \|^2\]
      \[
      \frac{\partial L}{\partial x} = 2 (\hat{x} - x) \left( \frac{\partial \hat{x}}{\partial x} - 1 \right)\]
     Minimum loss is achieved when \(\frac{\partial L}{\partial x} = 0\), or 
        \[
      2 (\hat{x} - x) \left( \frac{\partial \hat{x}}{\partial x} - 1 \right) = 0\]
        The obvious optimal solution for this is \(\hat{x} = x\)</p>

        <p><b>Case II: with \(L_1\) regularization</b>
          \[L = \| \hat{x} - x \|^2 + \lambda \| h \|_1\]

          \[\frac{\partial L}{\partial x} = 2 (\hat{x} - x) \left( \frac{\partial \hat{x}}{\partial x} - 1 \right) + \lambda \frac{\partial \| h \|_1}{\partial x}\]
          At the optimal point:
          \[
          \frac{\partial L}{\partial x} = 2 (\hat{x} - x) \left( W_d W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h)) - 1 \right) + \lambda W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h)) = 0
          \]

          
  
          Rearanging, we get:
          \[
          \hat{x} = x - \frac{\lambda W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h))}{2 \left( W_d W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h)) - 1 \right)}
          \]

          The reconstruction \(\hat{x}\) is shifted by a term that depends on the regularization parameter \(\lambda\) and the weights \(W_e\) and \(W_d\). This shift results in shrinkage and can even pull \(\hat{x}\) to zero in extreme cases.
  
  
        </div>

<h2>A simple illustration of "shrinkage"</h2>
<div class="l-page "> 
Let us see this reconstruction shrinkage in action: we will train an SAE on data sampled from a unit circle in 2D. 
The sliders vary hyperparameters for the SAE: the lambda controls the strength of the regularization, and the hidden dimension is the number of neurons in the output of the encoder or the input of the decoder (or equivalently the number of feature directions that the SAE can find).
At stronger sparsity regularization (larger lambda) the reconstructions do show a shrinkage effect, collapsing down to a point in an extreme case.
<br/>

<div style="display: flex;">
  <center>
  <div class="slider-container" >
    <label for="d-slider_0"><p>h_dim:</label> <span id="d-value_0">25</span>
    <input type="range" id="d-slider_0" min="0" max="8" step="1"  value="4" ></p>
    <label for="l-slider_0"><p>lambda:</label><span id="l-value_0">0.1</span>
    <input type="range" id="l-slider_0" min="0" max="12" step="1" value="4"></p>

    <p>Learned feature directions</p>

    <div id="feat_directions_0" style="border:1px solid black;margin:5px"></div>
  </div>
</center>

  <div>
    <center>
    <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
    <div id="scatterplot_0" style="border:1px solid black;margin:20px"></div>
  </center>

  </div>

    <div>
      <center>
      <p>Encoded activations [Hover over the plot to view]</p>
      <div id="heatmap_0"></div>
    </center>
    </div>
  </div>
</div>

<h2>A simple illustration of "splitting"</h2>
<div class="l-page "> 
  Large language models like ChatGPT and Claude encode lots of concepts (possibly of the order of millions or more) but the features recovered by the SAEs depend on the size of the hidden dimension. 
  If the SAE model size is not large enough, the model might have to "cut costs" (preferentially learn some directions instead of others). Let us construct an example that will build up to such a scenario.
  <h3>Case I</h3>

  Our toy dataset will be 3 "spokes" of data embedded in 2D, inspired by the works on toy models of superposition <dt-cite key="Elhage2022superposition"></dt-cite> and follow-up works <dt-cite key = "Sharkey2024superposition"></dt-cite>.
  Suppose each spoke represents a feature direction and points farther along a spoke indicate higher activation for that feature. Here are some subsets of such data used to train our models:
  <br/>
  <br/>

  <center>
  <img src="./img/3spokes.png" width="800px">
  </center>

  After the trained models can recover these directions, except in cases where the sparsity causes some directions to zero out completely, resulting in "dead features" (We'll come back to it later).
    <center>
    <img src="./img/3spokes_2.png" width="800px">
    </center>
  
  <h3>Case II</h3>

  Now consider a case where each of the three directions are split into 3 directions, a total of 9 directions: 

    <center>
    <img src="./img/3x3spokes.png" width="800px">
    </center>


  
  <div style="display: flex;">
    <center>
    <div class="slider-container" >
      <label for="d-slider_1"><p>h_dim:</label> <span id="d-value_1">3</span>
      <input type="range" id="d-slider_1" min="0" max="15" step="1"  value="1" ></p>
      <label for="l-slider_1"><p>lambda:</label><span id="l-value_1">0.3</span>
      <input type="range" id="l-slider_1" min="0" max="25" step="1" value="7"></p>
  
      <p>Learned feature directions</p>
  
      <div id="feat_directions_1" style="border:1px solid black;margin:5px"></div>
    </div>
  </center>

    <div>
      <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_1" style="border:1px solid black;margin:20px"></div>
    </center>

    </div>

      <div>
        <center>
        <p>Encoded activations [Hover over the plot to view]</p>
        <div id="heatmap_1"></div>
      </center>
      </div>
    </div>
  </div>
  </div>


<div class="l-page"> 
  <h2>When does feature splitting occur? <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-feature-splitting">[From Anthropic]</a> </h2>
    <img src="./img/splitting.png" width="800px">
  
  </div>



<div class="l-page"> 
  <h2>Gated SAEs</h2>
  [Coming soon]
</div>



<div class="l-page"> 
  <h2>K-Sparse AEs</h2>
  [Coming soon]
</div>

</dt-article>

<dt-appendix>
  <h4>Notes on training the SAE (the formatting may look weird depending on screen resolution)</h4>
  <p>[TODO] Incorporate model training tricks from works training LLM SAEs 
    <dt-cite key="Conmy2023"></dt-cite>
    <dt-cite key="Marks2023"></dt-cite>
    <dt-cite key="Nanda2023"></dt-cite>
    <dt-cite key="Anthropic2023"></dt-cite>  </p>
  <b>Reproducibility:</b>
  <dt-code block language="python">
    np.random.seed(42)
    torch.manual_seed(42)
  </dt-code>
  <b>Model definition:</b>
  <dt-code block language="python">
    class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(SparseAutoencoder, self).__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)  
        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)  
        self.b_dec = nn.Parameter(torch.zeros(input_dim))  
        self.b_enc = nn.Parameter(torch.zeros(hidden_dim))  
    
    def forward(self, x):
        # Encoder
        x_centered = x - self.b_dec 
        encoded = F.relu(self.encoder(x_centered) + self.b_enc)  
        
        # Decoder
        decoded = self.decoder(encoded) + self.b_dec  
        return decoded, encoded
  </dt-code>

  <b>Simplified training function. Note:</b>
  <p>1. All models were trained for 1000 epochs with Adam optimizer with lr = 0.001, with early stopping if the loss does not reduce for 200 epochs.</p>
  <p>2. After each optimizer step, decoder weights are normalized to have unit norm.</p>

  <dt-code block language="python">
  def train_sparse_autoencoder(model, dataloader, num_epochs, 
  lambda_l1, learning_rate=0.001, early_stop = True):
  
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    patience = 200  
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        for data in dataloader:
            inputs = data[0]
            
            outputs, encoded = model(inputs)
            mse_loss = criterion(outputs, inputs)
            
            l1_penalty = lambda_l1 * torch.sum(torch.abs(encoded))
            loss = mse_loss + l1_penalty
            epoch_loss += loss.item()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Normalize decoder weights to have unit norm
            with torch.no_grad():
                decoder_weights = model.decoder.weight.data
                norms = torch.norm(decoder_weights, dim=1, keepdim=True)
                model.decoder.weight.data = decoder_weights / norms
                
        epoch_loss /= len(dataloader)
        if early_stop:
            # Early stopping
            if epoch_loss < best_loss:
                best_loss = epoch_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f'Early stopping at epoch {epoch + 1}, Loss: {epoch_loss:.4f}')
                break

        
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')
  </dt-code>


  <b>Extract and plot feature directions, original and reconstructed data:</b>
  <dt-code block language="python">
  def extract_feature_directions(model):
      with torch.no_grad():
          decoder_weights = model.decoder.weight.data
          encoder_weights = model.encoder.weight.data

          # Each row of the weight matrix is a feature direction
          feature_directions = decoder_weights.clone()
          activations = encoder_weights.mean(axis=1).clone().cpu().numpy()

      return feature_directions.T, activations

  def plot_feature_directions(data, reconstructed, feature_directions, activations, title):
      range_x = [1.1 * data[:, 0].min(), 1.1 * data[:, 0].max()]
      range_y = [1.1 * data[:, 1].min(), 1.1 * data[:, 1].max()]

      fig = plt.figure(figsize=(6, 6))
      ax = fig.add_subplot()

      origin = np.array([0, 0, 0])
      ax.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10, label='Original data')
      ax.scatter(reconstructed[:, 0], reconstructed[:, 1], alpha=0.5, color='r', s=10, label='Reconstructed data')

      for i in range(len(activations)):
          ax.plot(
              [origin[0], feature_directions[i, 0] * activations[i]],
              [origin[1], feature_directions[i, 1] * activations[i]],
              color='limegreen', linewidth=3
          )
          ax.plot(
              [origin[0], -feature_directions[i, 0] * activations[i]],
              [origin[1], -feature_directions[i, 1] * activations[i]],
              color='limegreen', linewidth=3
          )

      feature_direction_proxy = plt.Line2D([0], [0], color='limegreen', linewidth=3)
      handles, labels = ax.get_legend_handles_labels()
      handles.append(feature_direction_proxy)
      labels.append('Feature directions')
      
      ax.legend(handles=handles, labels=labels, loc="upper right")
      plt.xticks([])
      plt.yticks([])
      plt.gca().set_aspect('equal')
      plt.xlim(range_x)
      plt.ylim(range_y)
      plt.savefig(title)
      plt.show()
  </dt-code>

  <b>Generate "Spokes" dataset:</b>
  <dt-code block language="python">
      def generate_spokes(num_vectors=3, num_points_per_vector=100, dim=3, scale_range=(0, 1)):

      # Generate random vectors in n dimensions
      vectors = np.random.randn(num_vectors, dim)
      
      # Initialize the list to collect points
      points = []
      
      for v in vectors:
          # Generate random scaling factors for the points along this vector
          scales = 2*np.random.random( num_points_per_vector)
          # Generate points along the vector
          for a in scales:
              points.append(a * v)
      
      # Convert the list of points to a numpy array
      points_array = np.array(points)
      
      return points_array.astype(np.float32)

  # Example usage
  data = generate_spokes(num_vectors=5, num_points_per_vector=200, dim=20, scale_range=(0, 5))
  </dt-code>

  <b>Generate multiple gaussians dataset:</b>
  <dt-code block language="python">
      def sample_from_mixture_with_labels(n_points, means, covariances, weights):
      
      n_gaussians = len(means)  # The number of Gaussians in the mixture
      d = means[0].shape[0]  # The dimensionality of the space
      
      # Validate inputs
      assert len(covariances) == n_gaussians, "Each Gaussian must have a covariance matrix."
      assert len(weights) == n_gaussians, "Each Gaussian must have a mixing weight."
      assert np.isclose(sum(weights), 1), "The mixing weights must sum to 1."
      
      # Choose which Gaussian each point will come from
      choices = np.random.choice(n_gaussians, size=n_points, p=weights)
      
      # Initialize an array to hold the generated samples
      samples = np.zeros((n_points, d))
      labels = np.zeros(n_points, dtype=int)
      
      # Generate samples from the chosen Gaussians
      for i in range(n_gaussians):
          # Number of points to sample from this Gaussian
          n_samples = np.sum(choices == i)
          
          if n_samples > 0:
              samples[choices == i] = np.random.multivariate_normal(
                  mean=means[i],
                  cov=covariances[i],
                  size=n_samples
              )
              labels[choices == i] = i
      
      return samples.astype(np.float32), labels


  #Example usage:
  n_points = 1000  # Number of points to generate
  dim = 20  # Dimensionality of the space

  means = [np.full(dim, -2), np.full(dim, 2),np.full(dim, 0)]  
  covariances = [np.eye(dim) * 1,  np.eye(dim) * 1,np.eye(dim) * 1] 
  weights = [0.33,0.33,0.34] 
  data, labels = sample_from_mixture_with_labels(n_points, means, covariances, weights)
  </dt-code>
  


  </dt-appendix>

<script type="text/bibliography">
  @article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
 }

 @misc{rajamanoharan2024improving,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{Conmy2023,
  author = {Arthur Conmy},
  title = {My best guess at the important tricks for training 1L SAEs},
  year = {2023},
  url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
  note = {Accessed: 2024-05-30}
}

@misc{Marks2023,
  author = {Sam Marks},
  title = {Some open-source dictionaries and dictionary learning infrastructure},
  year = {2023},
  url = {https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning},
  note = {Accessed: 2024-05-30}
}

@misc{Nanda2023,
  author = {Neel Nanda},
  title = {Open Source Replication and Commentary on Anthropic's Dictionary Learning Paper},
  year = {2023},
  url = {https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s},
  note = {Accessed: 2024-05-30}
}

@misc{Anthropic2023,
  author = {Anthropic},
  title = {Advice for Training Sparse Autoencoders},
  year = {2023},
  url = {https://transformer-circuits.pub/2023/monosemantic-features#appendix-autoencoder},
  note = {Accessed: 2024-05-30}
}


@misc{McDougall2023,
  author = {Callum McDougall},
  title = {Intro to Superposition & Sparse Autoencoders (Colab exercises)},
  year = {2023},
  url = {https://www.lesswrong.com/posts/LnHowHgmrMbWtpkxx/intro-to-superposition-and-sparse-autoencoders-colab },
  note = {Accessed: 2024-05-30}
}

@misc{gao2024scalingsae,
  title={Scaling and evaluating sparse autoencoders}, 
  author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  year={2024},
  eprint={2406.04093},
  archivePrefix={arXiv},
  primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{rajamanoharan2024gatedsae,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{bricken2023monosemanticity,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
  year={2023},
  journal={Transformer Circuits Thread},
  note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@misc{Wright2024shrinkage,
  title={Addressing Feature Suppression in SAEs}, 
  author={Benjamin Wright and Lee Sharkey},
  year={2024},
  url = {https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes},
  note = {Accessed: 2024-05-30}
}

@article{Elhage2022superposition,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year={2022},
  journal={Transformer Circuits Thread},
  note={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@misc{Sharkey2024superposition,
  title={[Interim research report] Taking features out of superposition with sparse autoencoders}, 
  author={Lee Sharkey, Dan Braun and Beren Millidge},
  year={2024},
  url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition#Toy_dataset_generation},
  note = {Accessed: 2024-05-30}
}
</script>

