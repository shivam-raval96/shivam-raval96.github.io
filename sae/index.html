<!doctype html>
<meta charset="utf-8">
<head>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js" ></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../distill.template.v1.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <link rel="stylesheet" href="styles.css">
  <script src="./makeplots.js"></script>
  <script src="./lrp-scatterplot.js"></script>

</head>

<script type="text/front-matter">
  title: "Toy SAE"
  description: "An exploration of sparse autoencoders trained on synthetic data"
  authors:
  - TBD: https://shivam-raval96.github.io/
  affiliations:
  - Harvard University 
</script>


<dt-article >
  <div class="l-page">
  <h1>Sparse autoencoders: the computational microscope to study the mind of a language model</h1>
    <h2>[Under construction. Note: the text is rough and some interactive visuals are still in the experimental phase and might be modified in the next iteration]</h2>
    <dt-byline></dt-byline>
    <h2>Outline</h2>

    <b>Part I. Exposition (disentangling polysemantic neurons in llms):</b>
    <ol>
      <li>Start with a real example: possibly an interactive visual using Gemma showing an example of a monosemantic neuron, a polysemantic neuron that activates on a few different features, and (maybe) an un-interpretable neuron, point to examples of Interpretability illusion</li>
      <li>Motivating the SAEs: why use them? What do we hope to gain from using them in an ideal scenario? Give an intuition of features, feature directions, sparsity, etc.</li>
    </ol>
    <b>Part II. Zooming In: What do the SAEs learn from toy datasets?</b>
    <ol>
        <li>What is an SAE?</li>
        <li>Some definitions, maybe some math?</li>
        <li>Show the most salient observations first, point to an expanded section for interested reader</li>
        <li>Start with a very toy and intuitive case that works perfectly in finding relevant features and reconstructing data</li>
        <li>Introduce the interesting phenomena in a Vanilla SAE  [Note to self: pick a few datasets where seeing these phenomena is intuitive]:</li>
    </ol>
    <b>Part III.  KSparse/Gated/JumpReLU SAEs:</b>
    <ol>
        <li>Give an intuition of why they work</li>
        <li>Show visuals of learned features in them vs vanilla SAE</li>
    </ol>
  <b>Part IV. Zooming out: back to LLMs</b>
  <ol>
    <li>(TBD) an LLM example (could be cherry-picked) at the end to compare vanilla vs K/Gated SAEs</li>
    <li>(TBD) an interactive visual with a UMAP of features where the relevant features light up as the user inputs text</li>
    <li>Open problems/challenges in training and interpreting SAEs</li>
    <li>Do SAEs find “true features”? Relevant post</li>
    <li>Refer to contemporary work? eg. 1, 2</li>
    <li>Future work & possible solutions</li>
  </ol>
  <dt-byline></dt-byline>
  <h1>Visuals</h1>

  <h2 id="linear-directions">Linear directions represent interpretable features in activations</h2>
  This is an example of linear directions of factual correctness for some datasets from Marks and Tegmark (2023) <dt-cite key = "marks2023geometrytruthemergentlinear"></dt-cite> . The experiments are done on Gemma-2-9B-it, 
  and an individual point corresponds to an activation from layer 35 post MLP from the model, produced using a text as input to the model. The data is visualized using the first two principal components. 
  Note that there is more structure in the data, with data points spread across different directions. We can only try to guess what the other directions mean! 
  SAEs offer a principled approach to decompose each datapoint into directions that correspond to the most basic features they encode, which can allow us to identify what other directions of variance correspond to.
  
  <div id="lrp-scatterplot"></div>
  These clusters represent the model's notion of what it believes is true! For eg. when asked whether the fact that the The earth is big and full of air, the model responds with: That's **true**! 
    
  The Earth is indeed very large, and it's surrounded by a layer of gases called the atmosphere, which is what we breathe and what we call "air". 

  <dt-byline></dt-byline>
  <h1>Content</h1>


  <h2>Part I. Exposition (disentangling polysemantic neurons in llms):</h2>
  <h3>A real example</h3>
  Interactive visual showing an example of a monosemantic neuron, a polysemantic neuron, and (maybe) an un-interpretable neuron
  [Coming Soon]
  <h3>Linear representation hypothesis, Polysematicity and Superposition</h3>
  [Add some pretty visuals]
  <p>There is growing amount of evidence that human-interpretable concepts (also referred to as features) are often linearly encoded 
  in the latent space of large language models (LLMs). This is commonly refered to as the linear represenation hypothesis <dt-cite key = "park2024linearrepresentationhypothesisgeometry,jiang2024originslinearrepresentationslarge,rajendran2024learninginterpretableconceptsunifying"></dt-cite>. For example, Burns et al.<dt-cite key ="burns2024discoveringlatentknowledgelanguage"></dt-cite> and Li et al. <dt-cite key = "li2024inferencetimeinterventionelicitingtruthful"></dt-cite> and Marks and Tegmark <dt-cite key = "marks2023geometrytruthemergentlinear"></dt-cite> find linear directions that represent truth, 
  Tigges et. al. <dt-cite key="tigges2023linearrepresentationssentimentlarge"></dt-cite> find linear representations of sentiment, Nanda et al. <dt-cite key="nanda2023emergentlinearrepresentationsworld"></dt-cite> find linear representation of player information game world of a language model trained to play Othello.
  These findings suggest that such complex features are represented surprisingly as linear directions in the high dimensional space of the language model's activations. 
  Such a linear representation of concepts is not limited to language models, and has been independently observed in Vision models <dt-cite key = "olah2018buildingblocks,goh2016thoughtvectors"></dt-cite>, Score-based generative models <dt-cite key = "wang2024conceptalgebrascorebasedtextcontrolled"></dt-cite> and
  multimodal Vison-Language models <dt-cite key = "trager2024linearspacesmeaningscompositional"></dt-cite>
  </p>
  <p><!-- Concepts like ... can be decomposed into linear combinations of independent features. -->
  Thus, groups of neurons together are responsible for encoding a certain feature, ie. they work in <i>Superposition</i>.
  Furthermore, since the network can learn to represent many more features than they have neurons, each neuron may contribute
  partially in representing many features at the same time. Such neurons are <i>Polysemantic</i>, 
  meaning they activate on serveral unrelated features at the same time <dt-cite key="bolukbasi2021interpretabilityillusionbert"></dt-cite>.</p>

  <p><b>[So what?]</b>
  One of the main goals in machine learning interpretability is to identify and disentange complex representations 
  and attribute the interpretable features for transparency, control and safety. 
  Given a vector of activation from a model, we would like to decompose it into its most basic features. 
  This is a difficult problem because we do not know a-priori what these features look like, or how many of them are there!
  
  One approach to finding these features is to reconstruct the feature vectors in an unsupervised way. If 
  model that learns this reconstuction process is large and sparse enough, the basis of this model would consist of the features we are looking for!</p>
  
  <h2>Part II. Sparse Autoencoders</h2>
  <p>Autoencoders (AE) generally consist of an encoder and a decoder, where the encoder maps the input to a latent space <b>[make a note that this is not the latent space of the llm]</b> and the decoder reconstructs the input from this latent space.
  One of the key objectives of an autoencoder is to capture the most important features of the data necessary to reconstruct it. 
  A sparse autoencoder (SAE) adds a sparsity penalty (L1 loss) to the loss function to learn sparse representations. 
  We would like to understand how these models work, what sort of features they can learn, and replicate some of the phenomena observed in large models ourselves.
  For our experiments we will train small models on synthetic data and study the trained models and the features they encode.</p>
  <b>Make a point that the decomposition happens to an overcomplete basis (more directions than neurons)</b>

  The model archetecture is very simple: the encoder is expands the high diemnsional inputs \(\mathbf{x} \in \mathbb{R}^d\), to an even higher dimension:
  \[
  \mathbf{h} = \text{ReLU}(W_{\text{enc}} (x - b_{\text{dec}}) + b_{\text{enc}})
  \]
  The decoder reconstructs the input and can be represented as:
  \[
  \mathbf{\hat{x}} =  W_{\text{dec}} \mathbf{h} + b_{\text{dec}}
  \]

  The loss is the sum of the error in reconstruction of the input (typically the MSE) and a sparsity penalty on L1 penalty on the activations \(\mathbf{h} \in \mathbb{R}^M\)
  \[
L = L_{\text{reconstruction}} + \lambda L_{\text{sparsity}} = \|\mathbf{x} - \mathbf{\hat{x}}\|^2_2 + \lambda \sum_{i=1}^M |h_i|
\]  

  

  <h3></h3>
  <dt-byline></dt-byline>
  <h1>Previous Content</h1>


  <h2>Introduction</h2>

    <p>The Anthropic interpretability team recently showed Sparse Autoencoders (SAEs) can extract interpretable 
      concepts from Claude 3 Sonnet<dt-cite key="templeton2024scaling"></dt-cite>. In the following weeks, 
      OpenAI <dt-cite key="gao2024scalingsae"></dt-cite> and  DeepMind <dt-cite key="templeton2024scaling"></dt-cite> showcased similar results using their own variant autoencoders, Gated SAEs (DeepMind) and K-Sparse AEs (OpenAI).
      
      
      We would like to understand how these models work, what sort of features they can learn, and replicate some of the phenomena observed in large models ourselves. 
      For our experiments we will train small models on synthetic data and mechanistically study the trained models and the features they encode.
      Two phenomena observed while training the vanilla SAEs with L1 sparsity are quite interesting: <a href="https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes">"Shrinkage"</a> <dt-cite key="Wright2024shrinkage"></dt-cite> and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-feature-splitting">"Splitting"</a> <dt-cite key="bricken2023monosemanticity"></dt-cite>.
    </p>

    

    <h2>Sparse Autoencoders as feature extractors</h2>
    Autoencoders (AE) generally consist of an encoder and a decoder, 
    where the encoder maps the input to a latent space and the decoder reconstructs the input from this 
    latent space. One of the key objectives of an autoencoder is to capture the most important features 
    of the data necessary to reconstruct it. A sparse autoencoder (SAE) adds a sparsity penalty (L1 loss) to the loss function to learn sparse representations. For our purposes, we want to decompose an input \( x^j \in \mathbb{R}^d \) into a sparse, linear combination of feature directions:
    \[
    x^j = x^j_0 + \sum_{i=1}^M f_i(x^j) d_i,
    \]
    where \( d_i \in \mathbb{R}^M \) are \( M \gg d \) are the feature directions, and the sparse coefficients \( f_i(x^j) \geq 0 \) are
    the corresponding feature activations for the directions. Since it is possible to arbitrarily reduce the sparsity loss term without affecting reconstructions, we might have to constrain the norms of the columns of \( W_{\text{dec}} \)
    during training, as suggested by previous work from DeepMind<dt-cite key="rajamanoharan2024improving"></dt-cite>. Thus, this will be our model: for some input \(\mathbf{x} \in \mathbb{R}^d\), the encoder is:
    \[
    \mathbf{h} = \text{ReLU}(W_{\text{enc}} (x - b_{\text{dec}}) + b_{\text{enc}})
    \]
    The decoder reconstructs the input and can be represented as:
    \[
    \mathbf{\hat{x}} =  W_{\text{dec}} \mathbf{h} + b_{\text{dec}}
    \]

    The loss is the sum of the error in reconstruction of the input (typically the MSE) and a sparsity penalty on L1 penalty on the activations \(\mathbf{h} \in \mathbb{R}^M\)
    \[
  L = L_{\text{reconstruction}} + \lambda L_{\text{sparsity}} = \|\mathbf{x} - \mathbf{\hat{x}}\|^2_2 + \lambda \sum_{i=1}^M |h_i|
  \]  

  <h2>What causes feature shrinkage?</h2>
    <p><b>Case I: No \(L_1\) regularization</b> \[L = \sum \| \hat{x} - x \|^2\]
      \[
      \frac{\partial L}{\partial \hat{x}} = 2 (\hat{x} - x)\]
     Minimum loss is achieved when \(\frac{\partial L}{\partial \hat{x}} = 0\).
        The obvious optimal solution for this is \(\hat{x} = x\) and the reconstruction matches the input.</p>

        <p><b>Case II: with \(L_1\) regularization</b>
          \[L = \| \hat{x} - x \|^2 + \lambda \| h \|_1\]

          \[
          \frac{\partial L}{\partial \hat{x}} = 2 (\hat{x} - x) + \lambda \frac{\partial \| h \|_1}{\partial \hat{x}} = 2 (\hat{x} - x) + \lambda W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h))
          \]

          At the optimal point:
          \[
          2 (\hat{x} - x) = - \lambda W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h))
          \]
          
  
          Rearanging, we get:
          \[
          \hat{x} = x - \frac{\lambda W_e \cdot \text{diag}(H(W_e (x - b_d) + b_h))}{2}
          \]

          The reconstruction \(\hat{x}\) is shifted by a term that depends on the regularization parameter \(\lambda\) and the weights \(W_e\) and \(W_d\). This shift results in shrinkage and can even pull \(\hat{x}\) to zero in extreme cases.
  
  
        </div>

<h2>A simple illustration of "shrinkage"</h2>
<div class="l-page "> 
Let us see this reconstruction shrinkage in action: we will train an SAE on data sampled from a unit circle in 2D. 
The sliders vary hyperparameters for the SAE: the lambda controls the strength of the regularization, and the hidden dimension is the number of neurons in the output of the encoder or the input of the decoder (or equivalently the number of feature directions that the SAE can find).
At stronger sparsity regularization (larger lambda) the reconstructions show the shrinkage effect, collapsing down to a point in an extreme case. Also, since the original data is symmetric, the best the model can do is find orthogonal directions to encode the data (it requires 4 directions to do that since we have used ReLU activation, which only allows positive activation magnitude).
<br/>

<div style="display: flex;">
  <center>
  <div class="slider-container" >
    <label for="d-slider_0"><p>h_dim:</label> <span id="d-value_0">25</span>
    <input type="range" id="d-slider_0" min="0" max="8" step="1"  value="4" ></p>
    <label for="l-slider_0"><p>lambda:</label><span id="l-value_0">0.1</span>
    <input type="range" id="l-slider_0" min="0" max="12" step="1" value="4"></p>
      <br/>
    <p>Learned feature directions</p>

    <div id="feat_directions_0" style="border:1px solid black;margin:5px"></div>
  </div>
</center>

  <div>
    <center>
    <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
    <div id="scatterplot_0" style="border:1px solid black;margin:20px"></div>
  </center>

  </div>

    <div>
      <center>
      <p>Encoded activations</p>
      <div id="barplot_0"></div>
      <p> * Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
    </center>
    </div>
  </div>
</div>

<h2>A simple illustration of "splitting"</h2>
<div class="l-page "> 
  Large language models like ChatGPT and Claude encode lots of concepts (possibly of the order of millions or more) but the features recovered by the SAEs depend on the size of the hidden dimension. 
  If the SAE model size is not large enough, the model might have to "cut costs" (preferentially learn some directions instead of others). Let us construct an example that will build up to such a scenario.
  <h3>Case I</h3>

  Our toy dataset will be 3 "spokes" of data embedded in 2D, inspired by the works on toy models of superposition <dt-cite key="Elhage2022superposition"></dt-cite> and follow-up works <dt-cite key = "Sharkey2024superposition"></dt-cite>.
  Suppose each spoke represents a feature direction and points farther along a spoke indicate higher activation for that feature. Here are some subsets of such data used to train our models:
  <br/>
  <br/>

  <center>
  <img src="./img/3spokes.png" width="800px">
  </center>

  After the trained models can recover these directions, except in cases where the sparsity causes some directions to zero out completely, resulting in "dead features" (We'll come back to it later).
    <center>
    <img src="./img/3spokes_2.png" width="800px">
    </center>
  
  <h3>Case II</h3>

  Now consider a case where each of the three directions are split into 3 directions, a total of 9 directions: 

    <center>
    <img src="./img/3x3spokes.png" width="800px">
    </center>


  
  <div style="display: flex;">
    <center>
    <div class="slider-container" >
      <label for="d-slider_1"><p>h_dim:</label> <span id="d-value_1">3</span>
      <input type="range" id="d-slider_1" min="0" max="15" step="1"  value="1" ></p>
      <label for="l-slider_1"><p>lambda:</label><span id="l-value_1">0.3</span>
      <input type="range" id="l-slider_1" min="0" max="25" step="1" value="7"></p>
  
      <p>Learned feature directions</p>
  
      <div id="feat_directions_1" style="border:1px solid black;margin:5px"></div>
    </div>
  </center>

    <div>
      <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_1" style="border:1px solid black;margin:20px"></div>
    </center>

    </div>

      <div>
        <center>
        <p>Encoded activations</p>
        <div id="barplot_1"></div>
        <p> * Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
      </center>
      </div>
    </div>
    <p> Note that here too, the structure of the data is symmetric, so the model is better off learning symmetric feature directions, in this case they are equiangular at angle 2\(\pi\)/3.</p>

  </div>
  </div>


<div class="l-page"> 
  <h2>When does feature splitting occur? <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-feature-splitting">[Placeholder: Excerpt from Anthropic's work]</a> </h2>
    <img src="./img/splitting.png" width="800px">
  
  </div>

  <div class="l-page"> 
    <h2>How does an SAE deal with features of different density?</h2>
    <p> Here is an example of four gaussian clusters with different densities and orientations. If the original data is not symmetric, then learning symmetric directions is not optimal for reconstruction (though it can learn them under high sparsity constraints, eg.  at h_dim = 3 and lambda = 1). But interestingly, model dedicates specific feature feature directions for higher density clusters.

    </p>
  <div style="display: flex;">
    <center>
    <div class="slider-container" >
      <label for="d-slider_2"><p>h_dim:</label> <span id="d-value_2">15</span>
      <input type="range" id="d-slider_2" min="0" max="5" step="1"  value="3" ></p>
      <label for="l-slider_2"><p>lambda:</label><span id="l-value_2">0.3</span>
      <input type="range" id="l-slider_2" min="0" max="9" step="1" value="1"></p>
  
      <p>Learned feature directions</p>
  
      <div id="feat_directions_2" style="border:1px solid black;margin:5px"></div>
    </div>
  </center>

    <div>
      <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_2" style="border:1px solid black;margin:20px"></div>
    </center>

    </div>

      <div>
        <center>
        <p>Encoded activations</p>
        <div id="barplot_2"></div>
        <p> * Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
      </center>
      </div>
    </div>
  </div>
  </div>
  </div>

  <div class="l-page"> 
    <h2>Dead Neurons and Resampling</h2>
    [Coming soon]
  </div>



<div class="l-page"> 
  <h2>K-Sparse AEs</h2>
  The main cause of shrinkage in reconstructions is the L1 penalty used to induce sparsity. A neat alternative is to directly
  set the number of active features used for reconstruction while zeroing out the rest. The encoder is now:
  \[
  \mathbf{h} = \text{TopK}(W_{\text{enc}} (x - b_{\text{dec}}) )
  \]
  

  The loss is just the MSE: 
  \(
L = \|\mathbf{x} - \mathbf{\hat{x}}\|^2_2 
\)

This is great! Since we do not rely on L1 penalty to induce sparsity, we have a better chance of getting good reconstructions. Let's look at the previous examples again:

<div style="display: flex;">
  <center>
    <p>Learned feature directions</p>
    <div id="feat_directions_3" style="border:1px solid black; margin:5px"></div>
  </center>

  <div>
    <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_3" style="border:1px solid black; margin:20px"></div>
    </center>
  </div>

  <div>
    <center>
      <p>Encoded activations</p>
      <div id="barplot_3"></div>
      <p>* Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
    </center>
  </div>
</div>

<div style="display: flex;">
  <center>
    <p>Learned feature directions</p>
    <div id="feat_directions_4" style="border:1px solid black; margin:5px"></div>
  </center>

  <div>
    <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_4" style="border:1px solid black; margin:20px"></div>
    </center>
  </div>

  <div>
    <center>
      <p>Encoded activations</p>
      <div id="barplot_4"></div>
      <p>* Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
    </center>
  </div>
</div>

<div class="l-page"> 
  Note: This hypothesis on feature splitting may be wrong; the results are not concrete and are subject to change after more experimentation and tinckering with loss function.
  In a vanilla SAE, if there is a harsh sparsity penalty or the SAE dimension is smaller than the data dimensionality, then the SAE can learn high-level features. 
  The way Anthropic trains their Vanilla SAEs, the model learns this hierarchy inherently to some extent, and so a larger model would find finer-level features. This might be fine, 
  but if one think of language as consisting of infinite concepts/features/true directions, but with sparsity that depends on the rarity of occurrence of the feature, the SAEs are being trained to find only O(10M) features amongst the full space of features, and which features they prioritize 
  and learn might be different for these variants, since we're clamping the number of active features in some way, and it might not be clear what features they choose to learn/learn first.
  </div>

<div style="display: flex;">
  <center>
    <p>Learned feature directions</p>
    <div id="feat_directions_5" style="border:1px solid black; margin:5px"></div>
  </center>

  
  <div>
    <center>
      <p><span style="color:steelblue">Original data</span> along with its <span style="color:#ff6666">reconstructions</span></p>
      <div id="scatterplot_5" style="border:1px solid black; margin:20px"></div>
    </center>
  </div>

  <div>
    <center>
      <p>Encoded activations</p>
      <div id="barplot_5"></div>
      <p>* Maximum value for a neuron is shown in grey. Hover on the individual bars to highlight points that have high magnitude for that dimension.</p>
    </center>
  </div>
</div>
</div>

</dt-article>

<dt-appendix>
  <h2>Outline</h2>
  <b>Part I. Motivating the SAEs: Why Use Them?</b>
  <p>What do we hope to gain from using them in an ideal scenario? Give an intuition of features, feature directions, sparsity, etc.</p>
  <b>Part II. Zooming In: What do the SAEs learn from toy datasets?</b>
  <ol>
      <li>What is an SAE?</li>
      <li>Some definitions, maybe some math?</li>
      <li>
          Start with a very toy and intuitive case that works perfectly in finding relevant features and reconstructing data:
          <ol type="i">a. Possibly relevant work: <a href="https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition">Ref1</a>, <a href="https://arxiv.org/pdf/2211.09169">Ref2</a> </ol>
          <ol type="i">b. ...</ol>

      </li>
      <li>
        Introduce the issues in a Vanilla SAE: 
        <ol type="i">a. Dead neruons</ol>
        <ol type="i">b. Shrinkage</ol>
        <ol type="i">c. ...</ol>
    </li>
    <li>
      Possible ways to address these issues: 
      <ol type="i">a. Resampling, <a href="https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning-resampling">ghost grads</a> (Note to self: no longer relevant), aux loss for dead neurons</ol>
      <ol type="i">b. KSparse/Gated SAEs for shrinkage, <a href="https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes">relevant post</a></ol>
      <ol type="i">c. ...</ol>
  </li>
  <li>Feature splitting</li>
  <li>
    How do the SAEs learn features when the data contain 
    <ol type="i">a. features with different levels of sparsity/density</ol>
    <ol type="i">b. continuous features, discrete/binary features or a mixture of both</ol>
    <ol type="i">c. correlations between features (<a href="https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy">similar work</a>)</ol>
    <ol type="i">d. certain rules and hierarchy</ol>
    <ol type="i">e. fractal structure</ol>
    <ol type="i">f. outliers?</ol>
    <ol type="i">g. Lies on a manifold, high dimensional curve (Note to self: look in Lissajous curves)</ol>
    <ol type="i">h. (TBD) feature multiplicity: the appearance of multiple distinct feature vectors that correspond to the same concept in different contexts (Martin and Fernanda’s compositionality paper)</ol>
    <ol type="i">i. (TBD, most like out of scope due to high level of complexity) A complicated combination of a subset of these (eg. <a href="https://arxiv.org/pdf/2305.13673">hierarchical linguistic structures</a> , <a href="https://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf">PCFG</a>)</ol>  
</li>
<li>
  Comparison of Vanilla SAEs, KSparse SAEs, Gated SAEs 
  <ol type="i">Show reconstructions and learned features for a range of datasets from above</ol>
  <ol type="i">Comment about traning speed?</ol>
</li>
</ol>

<b> Zooming out: SAEs for LLMs</b>
<ol>
<li>(Possibly) an interactive visual with a UMAP of features where the relevant features light up as the user inputs text</li>
<li>Open problems/challenges in training and interpreting SAEs </li>
<li>Do SAEs find “true features”? <a href="https://www.lesswrong.com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features">Relevant post</a></li>
<li>Refer to contemporary work: <a href="https://www.lesswrong.com/posts/rZPiuFxESMxCDHe4B/sae-reconstruction-errors-are-empirically-pathological">Ref1</a>, <a href="https://www.lesswrong.com/posts/xzJK3nENopiLmo77H/identifying-functionally-important-features-with-end-to-end">Ref2</a> </ol></li>
<dt-byline></dt-byline>

  
  
      
      
      
      <li>
          Start with a very toy and intuitive case that works perfectly in finding relevant features and reconstructing data:
          <ol type="i">a. Possibly relevant work: <a href="https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition">Ref1</a>, <a href="https://arxiv.org/pdf/2211.09169">Ref2</a> </ol>
          <ol type="i">b. ...</ol>

      </li>
      <li>
        Introduce the issues in a Vanilla SAE: 
        <ol type="i">a. Dead neruons</ol>
        <ol type="i">b. Shrinkage</ol>
        <ol type="i">c. ...</ol>
    </li>
    <li>
      Possible ways to address these issues: 
      <ol type="i">a. Resampling, <a href="https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning-resampling">ghost grads</a> (Note to self: no longer relevant), aux loss for dead neurons</ol>
      <ol type="i">b. KSparse/Gated SAEs for shrinkage, <a href="https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes">relevant post</a></ol>
      <ol type="i">c. ...</ol>
  </li>
  <li>Feature splitting</li>
  <li>
    How do the SAEs learn features when the data contain 
    <ol type="i">a. features with different levels of sparsity/density</ol>
    <ol type="i">b. continuous features, discrete/binary features or a mixture of both</ol>
    <ol type="i">c. correlations between features (<a href="https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy">similar work</a>)</ol>
    <ol type="i">d. certain rules and hierarchy</ol>
    <ol type="i">e. fractal structure</ol>
    <ol type="i">f. outliers?</ol>
    <ol type="i">g. Lies on a manifold, high dimensional curve (Note to self: look in Lissajous curves)</ol>
    <ol type="i">h. (TBD) feature multiplicity: the appearance of multiple distinct feature vectors that correspond to the same concept in different contexts (Martin and Fernanda’s compositionality paper)</ol>
    <ol type="i">i. (TBD, most like out of scope due to high level of complexity) A complicated combination of a subset of these (eg. <a href="https://arxiv.org/pdf/2305.13673">hierarchical linguistic structures</a> , <a href="https://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf">PCFG</a>)</ol>  
</li>
<li>
  Comparison of Vanilla SAEs, KSparse SAEs, Gated SAEs 
  <ol type="i">Show reconstructions and learned features for a range of datasets from above</ol>
  <ol type="i">Comment about traning speed?</ol>
</li>
</ol>

<b> Zooming out: SAEs for LLMs</b>
<ol>
<li>(Possibly) an interactive visual with a UMAP of features where the relevant features light up as the user inputs text</li>
<li>Open problems/challenges in training and interpreting SAEs </li>
<li>Do SAEs find “true features”? <a href="https://www.lesswrong.com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features">Relevant post</a></li>
<li>Refer to contemporary work: <a href="https://www.lesswrong.com/posts/rZPiuFxESMxCDHe4B/sae-reconstruction-errors-are-empirically-pathological">Ref1</a>, <a href="https://www.lesswrong.com/posts/xzJK3nENopiLmo77H/identifying-functionally-important-features-with-end-to-end">Ref2</a> </ol></li>
<dt-byline></dt-byline>

  <h4>Notes on training the SAE (the formatting may look weird depending on screen resolution)</h4>
  <p>[TODO] Incorporate model training tricks from works training LLM SAEs 
    <dt-cite key="Conmy2023"></dt-cite>
    <dt-cite key="Marks2023"></dt-cite>
    <dt-cite key="Nanda2023"></dt-cite>
    <dt-cite key="Anthropic2023"></dt-cite>  </p>
  <b>Reproducibility:</b>
  <dt-code block language="python">
    def set_seed(seed):
        np.random.seed(seed)
        random.seed(seed)
        t.manual_seed(seed)
        t.cuda.manual_seed(seed)
        t.cuda.manual_seed_all(seed)
        t.backends.cudnn.deterministic = True
        t.backends.cudnn.benchmark = False

    set_seed(42)
  </dt-code>
  <b>Model definition</b> (Also refer to <a href="https://colab.research.google.com/drive/15S4ISFVMQtfc0FPi29HRaX03dWxL65zx?usp=sharing"> this colab notebook</a> for detailed exercises on defining and training SAEs)
  <dt-code block language="python">
    @dataclass
    class SparseAutoEncoderConfig:
        n_instances: int
        n_input_ae: int
        n_hidden_ae: int
        lambda_coeff: float = 0.5
        tied_weights: bool = False
        reg_power: float = 1.0
        activation: str = 'relu'
        standardize_data: bool = True
        reg_type: str = 'l1'
        k:int = 2
    
    class SparseAutoEncoder(nn.Module):
        W_enc: Float[Tensor, "n_instances n_input_ae n_hidden_ae"]
        W_dec: Float[Tensor, "n_instances n_hidden_ae n_input_ae"]
        b_enc: Float[Tensor, "n_instances n_hidden_ae"]
        b_dec: Float[Tensor, "n_instances n_input_ae"]
            
        def __init__(self, cfg: SparseAutoEncoderConfig, device='cpu'):
            super().__init__()
            self.cfg = cfg
            self.device = device
            self.istopk = False
    
            
            # Initialize encoder weights using Xavier normal initialization
            self.W_enc = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_input_ae, cfg.n_hidden_ae))))
            
            # Initialize decoder weights (if not using tied weights)
            if not cfg.tied_weights:
                self.W_dec = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden_ae, cfg.n_input_ae))))
            else:
                self.W_dec = None
            
            # Initialize biases    
            self.b_enc = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden_ae))
            self.b_dec = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_input_ae))
            
            # Set activation function based on configuration
            self.act = F.relu
            if cfg.activation =='sigmoid':
                self.act = F.sigmoid
                        
            if cfg.reg_type == 'topk':
                self.istopk = True
                self.k = self.cfg.k
            # Move model to specified device
            self.to(device)
            
        def forward(self, x: Float[Tensor, "batch_size n_instances n_input_ae"]):
            # Center the input
            x_centered = x - self.b_dec
            
            # Encoder: compute hidden representation
            h = einops.einsum(
                x_centered, self.W_enc, 
                "batch_size n_instances n_input_ae, n_instances n_input_ae n_hidden_ae -> batch_size n_instances n_hidden_ae")
            
            # Apply activation function to get embedded representation
            act = h + self.b_enc
            if self.istopk:
                topk_values, topk_indices = t.topk(act, self.k, dim=-1)
                mask = t.zeros_like(act, dtype=t.bool)
                mask.scatter_(-1, topk_indices, 1)
                embedded = act * mask.float()
            else:
                embedded = self.act(act)
            
            # Decoder: reconstruct input
            x_reconstructed = einops.einsum(
                embedded, (self.W_enc.transpose(-1, -2) if self.cfg.tied_weights else self.W_dec), 
                "batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae") + self.b_dec
            
            # Compute losses
            sparsity_loss = t.zeros_like(embedded).sum(-1)
            mse = ((x_reconstructed - x)**2).mean(-1)  # Mean squared error
            if not self.istopk:
                sparsity_loss = (embedded**self.cfg.reg_power).sum(-1)  # Sparsity regularization
            l0_loss = t.sum(embedded != 0, axis = [0,2])/embedded.shape[0]  # L0 "norm" (fraction of non-zero activations)
            
            # Combine losses
            loss = (mse + self.cfg.lambda_coeff * sparsity_loss).mean(0).sum()
    
            return loss, mse, sparsity_loss, embedded, x_reconstructed, l0_loss
        
        @t.no_grad()
        def normalize_decoder(self) -> None:
            # Normalize decoder weights to unit norm
            if self.cfg.tied_weights:
                self.W_enc.data = self.W_enc.data / self.W_enc.data.norm(dim=1, keepdim=True)
            else:
                self.W_dec.data = self.W_dec.data / self.W_dec.data.norm(dim=2, keepdim=True)
    
        @t.no_grad()
        def resample_neurons(
            self,
            h: Float[Tensor, "batch_size n_instances n_hidden"],
            frac_active_in_window: Float[Tensor, "window n_instances n_hidden_ae"],
            neuron_resample_scale: float,
        ) -> None:
            '''
            Resamples neurons that have been dead for `dead_neuron_window` steps, according to `frac_active`.
            '''
            # Create a mask for dead neurons
            dead_features_mask = t.empty((self.cfg.n_instances, self.cfg.n_hidden_ae), dtype=t.bool, device=self.W_enc.device)
    
            for instance in range(self.cfg.n_instances):
                # Identify dead neurons (those that haven't been active in the window)
                is_dead = (frac_active_in_window[:, instance].sum(0) < 1e-8)
                dead_features_mask[instance] = is_dead
                dead_features = t.nonzero(is_dead).squeeze(-1)
                n_dead = dead_features.numel()
                if n_dead == 0: continue
    
                # Generate new random values for dead neurons
                replacement_values = t.randn((n_dead, self.cfg.n_input_ae), device=self.W_enc.device)
                replacement_values_normalized = replacement_values / (replacement_values.norm(dim=-1, keepdim=True) + 1e-8)
    
                # Replace weights for dead neurons
                self.W_dec.data[instance, dead_features, :] = replacement_values_normalized
                self.W_enc.data[instance, :, dead_features] = replacement_values_normalized.T
                self.b_enc.data[instance, dead_features] = 0.0
    
        def get_test_loss(self, batch_size):
            # Generate a batch of test data
            x, _ = self.data_generator.generate_batch(batch_size, self.cfg.n_instances)
            x = x.permute(1, 0, 2)
            
            # Compute loss on test data
            loss, mse, sparsity_loss, embedded, x_reconstructed, l0_loss = self.forward(x)
            
            return loss, mse, sparsity_loss, l0_loss
        
        def extract_feature_directions(self, runs):
            d = self.cfg.n_hidden_ae
        
            # Accumulate activations over multiple runs
            activations = t.zeros([1, d])
            for i in range(runs):
                n = 256
                x, _ = self.data_generator.generate_batch(n, 1,)
                x = x.permute(1, 0, 2)
                _, _, _, embedded, x_reconstructed, l0_loss = autoencoder.forward(x)
                activations += embedded.mean(axis=[0])
            return activations / runs
    
        def optimize(
            self,
            data_generator: DataGenerator,
            batch_size: int = 1024,
            steps: int = 10_000,
            log_freq: int = 100,
            lr: float = 1e-3,
            lr_scale: Callable[[int, int], float] = lambda step, steps: 1.0,
            neuron_resample_window: Optional[int] = None,
            dead_neuron_window: Optional[int] = None,
            neuron_resample_scale: float = 0.2,
        ):
            self.data_generator = data_generator
            # Validation check for neuron resampling parameters
            if neuron_resample_window is not None:
                assert (dead_neuron_window is not None) and (dead_neuron_window < neuron_resample_window)
    
            # Initialize optimizer
            optimizer = t.optim.Adam(list(self.parameters()), lr=lr)
            frac_active_list = []
            pbar = tqdm(range(steps))
    
            # Initialize data logging
            data_log = {"feature_directions": [], "l0_loss": [], "MSE_loss": [], "SP_loss": []}
            #colors = None
            
            for step in pbar:
                self.normalize_decoder()
                
                # Resample dead neurons if needed
                if (neuron_resample_window is not None) and ((step + 1) % neuron_resample_window == 0):
                    frac_active_in_window = t.stack(frac_active_list[-neuron_resample_window:], dim=0)
                    batch, _ = data_generator.generate_batch(batch_size, self.cfg.n_instances)
                    batch = batch.permute(1, 0, 2)
                    _, _, _, embedded, _, _ = self.forward(x)
                    self.resample_neurons(embedded, frac_active_in_window, neuron_resample_scale)
    
                # Generate batch and prepare input
                x, _ = self.data_generator.generate_batch(batch_size, self.cfg.n_instances)
                x = x.permute(1, 0, 2)
                
                # Update learning rate
                step_lr = lr * lr_scale(step, steps)
                for group in optimizer.param_groups:
                    group['lr'] = step_lr
                
                # Perform optimization step
                optimizer.zero_grad()
                loss, mse, sparsity_loss, embedded, x_reconstructed, l0_loss = self.forward(x)
                loss, mse, sparsity_loss, l0_loss = self.get_test_loss(batch_size)
                loss.backward()
                optimizer.step()
                
                # Calculate fraction of active neurons
                frac_active = einops.reduce((embedded.abs() > 1e-8).float(), "batch_size instances hidden_ae -> instances hidden_ae", "mean")
                frac_active_list.append(frac_active)
                
                # Log data periodically
                if step % log_freq == 0 or (step + 1 == steps):
                    act = self.extract_feature_directions(100)
                    data_log["feature_directions"].append((self.W_enc*act).detach().cpu().numpy())
                    pbar.set_postfix(loss=loss.item(), lr=step_lr)
                    data_log["l0_loss"].append(l0_loss.detach().cpu().numpy())
                    data_log["MSE_loss"].append(mse.mean(axis=0).unsqueeze(0).detach().cpu().numpy())
                    data_log["SP_loss"].append(sparsity_loss.mean(axis=0).unsqueeze(0).detach().cpu().numpy())
                    data_log["titles"].append(f"Step {step}/{steps}")
                
            return data_log
  </dt-code>

  <b>Note:</b>
  <p>1. All models were trained for 22000 epochs with Adam optimizer with lr = 0.001.</p>
  <p>2. After each optimizer step, decoder weights are normalized to have unit norm.</p>
  <p>3. Every 2500 epochs, neurons that have been dead for 400 epochs are resampled with probability 0.8</p>


  <b>Dataset Generator:</b>
  <dt-code block language="python">
    @dataclass
    class DatasetConfig:
        n_points: int = 1000  # Total number of points to generate
        n_input_ae: int = 3   # Dimensionality of input data (3 for 3D data)
        dataset: str = 'circle'  # Type of dataset to generate
        n_lines: int = 5  # Number of lines/spokes in certain datasets
    
    class DataGenerator:
        def __init__(self, cfg: DatasetConfig, device: str = 'cpu', seed: int = None):
            """
            Initialize the DataGenerator with configuration settings.
    
            Args:
                cfg (DatasetConfig): Configuration for dataset generation
                device (str): Device to use for tensor operations
                seed (int): Seed for random number generation
            """
            self.cfg = cfg
            self.device = device
            self.n_lines = self.cfg.n_lines
            self.dataset = cfg.dataset
            
            # Set random seed for reproducibility if provided
            if seed is not None:
                np.random.seed(seed)
                t.manual_seed(seed)
    
            # Dictionary mapping dataset names to their respective generation functions
            self.data_generators = {
                'circle': self.circle,
                'spokes': self.spokes,
                'bunchedspokes': self.bunched_spokes,
                'spokesabs3d': self.spokes_abs3d,
                'swissroll': self.swiss_roll,
                'scurve': self.s_curve,
                'rand_unitnorm': self.rand_unit_norm,
                'sparsebinary': self.sparse_binary,
                'multiplegaussians': self.multiple_gaussians
    
            }
    
            if self.dataset not in self.data_generators:
                raise ValueError(f"Unknown dataset type: {self.dataset}")
    
            self.data_generator = self.data_generators[self.dataset]
    
        def generate_batch(self, batch_size: int, n_instances: int, **kwargs) -> tuple[t.Tensor, np.ndarray]:
            """
            Generate a batch of data instances.
    
            Args:
                batch_size (int): Number of points per instance
                n_instances (int): Number of instances to generate
                **kwargs: Additional arguments for the data generator
    
            Returns:
                tuple: (Tensor of data points, array of labels)
            """
            data_list, label_list = zip(*[self.data_generator(num_points=batch_size, **kwargs) for _ in range(n_instances)])
            self.data = t.stack(data_list)
            return self.data, np.array(label_list)
    
        def circle(self, num_points: int = 100, noise_level: float = 0.01, radius: float = 1.0) -> t.Tensor:
            """
            Generate points along a 2D circle with added Gaussian noise.
    
            Args:
                num_points (int): Number of points to generate
                noise_level (float): Standard deviation of the Gaussian noise
                radius (float): Radius of the circle
    
            Returns:
                t.Tensor: Tensor of shape (num_points, 2) containing the noisy circle points
            """
            theta = np.linspace(0, 2 * np.pi, num_points)
            x = radius * np.cos(theta)
            y = radius * np.sin(theta)
            circle_points = np.stack((x, y), axis=-1)
            noise = np.random.normal(scale=noise_level, size=circle_points.shape)
            noisy_circle_points = circle_points + noise
            return t.tensor(noisy_circle_points.astype(np.float32), device=self.device)
    
        def spokes(self, num_points: int = 50, num_lines: int = 5, sparsity: float = 0.0, offset: float = 0, sameMaxDist: bool = True) -> tuple[t.Tensor, Dict[str, Any]]:
            """
            Generate sparse points along equiangular lines in 2D.
    
            Args:
                num_points (int): Total number of points to generate
                num_lines (int): Number of equiangular lines
                sparsity (float): Fraction of points that should be zero
                offset (float): Angular offset for the lines
                sameMaxDist (bool): If True, all points have the same max distance from origin
    
            Returns:
                tuple: (Tensor of points, dict with labels and colors)
            """
            if not 0 <= sparsity < 1:
                raise ValueError("Sparsity must be between 0 and 1")
            if num_points <= num_lines:
                raise ValueError("Number of points must be greater than number of lines")
    
            angles = np.linspace(offset, 2 * np.pi + offset, num_lines, endpoint=False)
            num_zero_points = int(num_points * sparsity)
            num_nonzero_points = num_points - num_zero_points
    
            points = []
            labels = []
    
            # Ensure each line has at least one point
            points_per_line = np.full(num_lines, 1)
            remaining_points = num_nonzero_points - num_lines
    
            # Distribute remaining points randomly among lines
            while remaining_points > 0:
                chosen_line = np.random.choice(num_lines)
                points_per_line[chosen_line] += 1
                remaining_points -= 1
    
            # Generate non-zero points along each line
            for i, angle in enumerate(angles):
                num_points_on_line = points_per_line[i]
                max_distance = 1 if sameMaxDist else np.random.rand() * 2
                for _ in range(num_points_on_line):
                    r = np.random.rand() * max_distance
                    x = r * np.cos(angle)
                    y = r * np.sin(angle)
                    points.append([x, y])
                    labels.append(i)
    
            points = np.array(points)
            labels = np.array(labels)
    
            # Add zero points
            zero_points = np.zeros((num_zero_points, 2))
            zero_labels = np.full(num_zero_points, -1)
            all_points = np.vstack([points, zero_points])
            all_labels = np.concatenate([labels, zero_labels])
    
            # Shuffle the points and labels together
            indices = np.arange(len(all_points))
            np.random.shuffle(indices)
            all_points = all_points[indices]
            all_labels = all_labels[indices]
    
            return t.tensor(all_points.astype(np.float32), device=self.device), {'labels': all_labels, 'colors': []}
    
        def bunched_spokes(self, num_points: int = 50, main_num_lines: int = 3, close_lines_multiplier: int = 1, angle_offset: float = 0.4, sparsity: float = 0.0, sameMaxDist: bool = True) -> tuple[t.Tensor, Dict[str, Any]]:
            """
            Generate sparse points along main equiangular lines with additional close lines in 2D.
    
            Args:
                num_points (int): Total number of points to generate
                main_num_lines (int): Number of main equiangular lines
                close_lines_multiplier (int): Number of lines close to each main line
                angle_offset (float): Angular offset for the close lines
                sparsity (float): Fraction of points that should be zero
                sameMaxDist (bool): If True, all points have the same max distance from origin
    
            Returns:
                tuple: (Tensor of points, dict with labels and colors)
            """
            if not 0 <= sparsity < 1:
                raise ValueError("Sparsity must be between 0 and 1")
    
            total_lines = main_num_lines * (1 + close_lines_multiplier * 2)
            main_angles = np.linspace(0, 2 * np.pi, main_num_lines, endpoint=False)
    
            # Generate angles for all lines (main and close)
            all_angles = []
            for angle in main_angles:
                all_angles.append(angle)
                for i in range(1, close_lines_multiplier + 1):
                    all_angles.append(angle + i * angle_offset)
                    all_angles.append(angle - i * angle_offset)
            all_angles = np.array(all_angles)
    
            num_zero_points = int(num_points * sparsity)
            num_nonzero_points = num_points - num_zero_points
    
            points_per_line = num_nonzero_points // total_lines
            extra_points = num_nonzero_points % total_lines
    
            points = []
            colors = {}
            labels = []
            base_colors = list(mcolors.TABLEAU_COLORS.values())
    
            # Generate non-zero points along each line
            for i, angle in enumerate(all_angles):
                main_line_index = i // (1 + 2 * close_lines_multiplier)
                base_color = base_colors[main_line_index % len(base_colors)]
    
                # Create a color variant with decreasing alpha for close lines
                color_variant = mcolors.to_rgba(base_color)
                alpha = 1.0 - (0.05 * (i % (1 + 2 * close_lines_multiplier)))
                variant_color = (color_variant[0], color_variant[1], color_variant[2], alpha)
    
                num_points_on_line = points_per_line + (1 if i < extra_points else 0)
                max_distance = 1 if sameMaxDist else np.random.rand() * 2
    
                for _ in range(num_points_on_line):
                    r = np.random.rand() * max_distance
                    x = r * np.cos(angle)
                    y = r * np.sin(angle)
                    points.append([x, y])
                    
                    if i not in colors:
                        colors[i] = variant_color
                    labels.append(i)
    
            points = np.array(points)
            labels = np.array(labels)
    
            # Add zero points
            zero_points = np.zeros((num_zero_points, 2))
            all_points = np.vstack([points, zero_points])
    
            # Shuffle the points and labels together
            indices = np.arange(len(all_points))
            np.random.shuffle(indices)
            all_points = all_points[indices]
            labels = labels[indices]
    
            return t.tensor(all_points.astype(np.float32), device=self.device), {'labels': labels, 'colors': colors}
    
        def spokes_abs3d(self, num_points: int = 50, num_lines: int = 5, sparsity: float = 0.0, offset: float = 0, sameMaxDist: bool = True, v_scale: float = 1.0) -> Tuple[t.Tensor, Dict[str, Any]]:
            """
            Generate sparse points along equiangular lines in 3D, with a V-shaped absolute curve in the third dimension.
    
            Args:
                num_points (int): Total number of points to generate
                num_lines (int): Number of equiangular lines
                sparsity (float): Fraction of points that should be zero
                offset (float): Angular offset for the lines
                sameMaxDist (bool): If True, all points have the same max distance from origin in xy-plane
                v_scale (float): Scale factor for the V-shaped curve in the third dimension
    
            Returns:
                tuple: (Tensor of 3D points, dict with labels and colors)
            """
            if not 0 <= sparsity < 1:
                raise ValueError("Sparsity must be between 0 and 1")
            if num_points <= num_lines:
                raise ValueError("Number of points must be greater than number of lines")
    
            angles = np.linspace(offset, 2 * np.pi + offset, num_lines, endpoint=False)
            num_zero_points = int(num_points * sparsity)
            num_nonzero_points = num_points - num_zero_points
    
            points = []
            labels = []
    
            # Ensure each line has at least one point
            points_per_line = np.full(num_lines, 1)
            remaining_points = num_nonzero_points - num_lines
    
            # Distribute remaining points randomly among lines
            while remaining_points > 0:
                chosen_line = np.random.choice(num_lines)
                points_per_line[chosen_line] += 1
                remaining_points -= 1
    
            # Generate non-zero points along each line
            for i, angle in enumerate(angles):
                num_points_on_line = points_per_line[i]
                max_distance = 1 if sameMaxDist else np.random.rand() * 2
                for _ in range(num_points_on_line):
                    r = np.random.rand() * max_distance
                    x = r * np.cos(angle)
                    y = r * np.sin(angle)
                    
                    # Add V-shaped absolute curve in the third dimension
                    z = v_scale * np.abs(r - 0.5)
                    
                    points.append([x, y, z])
                    labels.append(i)
    
            points = np.array(points)
            labels = np.array(labels)
    
            # Add zero points
            zero_points = np.zeros((num_zero_points, 3))
            zero_labels = np.full(num_zero_points, -1)
            all_points = np.vstack([points, zero_points])
            all_labels = np.concatenate([labels, zero_labels])
    
            # Shuffle the points and labels together
            indices = np.arange(len(all_points))
            np.random.shuffle(indices)
            all_points = all_points[indices]
            all_labels = all_labels[indices]
    
            return t.tensor(all_points.astype(np.float32), device=self.device), {'labels': all_labels, 'colors': []}
        
        
        def swiss_roll(self, num_points=1000, noise=0.0, random_state=None, hole = True):
            """
            Generate a Swiss roll dataset.
    
            Parameters:
            - n_samples: int, number of sample points
            - noise: float, standard deviation of Gaussian noise
            - random_state: int, random seed for reproducibility
            - hole: bool, add a hole in the manifold
    
    
            Returns:
            - data: ndarray of shape (n_samples, 3)
            - metadata: dict containing 't' and 'color' parameters
            """
            data, w = make_swiss_roll(n_samples=num_points, noise=noise, random_state=random_state, hole=hole)
    
    
            metadata = {'color': w}
    
            return t.tensor(data.astype(np.float32), device=self.device), metadata
        
        def s_curve(self, num_points=1000, noise=0.0, random_state=None):
            """
            Generate a Swiss roll dataset.
    
            Parameters:
            - n_samples: int, number of sample points
            - noise: float, standard deviation of Gaussian noise
            - random_state: int, random seed for reproducibility
    
            Returns:
            - data: ndarray of shape (n_samples, 3)
            - metadata: dict containing 't' and 'color' parameters
            """
            data, w = make_s_curve(n_samples=num_points, noise=noise, random_state=random_state)
    
    
            metadata = {'color': w}
    
            return t.tensor(data.astype(np.float32), device=self.device), metadata
        
        
        def rand_unit_norm(self, num_points=1000, n_features=5, random_state=None):
            """
            Generate a dataset where all points have unit norm.
    
            Parameters:
            - n_samples: int, number of sample points
            - n_features: int, number of features
            - random_state: int, random seed for reproducibility
    
            Returns:
            - data: ndarray of shape (n_samples, n_features)
            - metadata: dict containing original unnormalized data
            """
            np.random.seed(random_state)
    
            # Generate random data
            data_raw = np.random.randn(num_points, n_features)
    
            # Normalize each sample to unit norm
            norms = np.linalg.norm(data_raw, axis=1, keepdims=True)
            data = data_raw / norms
    
            metadata = {}
    
            return t.tensor(data.astype(np.float32), device=self.device), metadata
        
        
        def sparse_binary(self,num_points=1000, n_features=5, sparsity=0.3, random_state=None):
            """
            Generate a dataset with sparse binary features.
    
            Parameters:
            - n_samples: int, number of sample points
            - n_features: int, number of features
            - sparsity: float, expected fraction of non-zero entries
            - random_state: int, random seed for reproducibility
    
            Returns:
            - data: ndarray of shape (n_samples, n_features) with binary entries
            - metadata: dict containing sparsity information
            """
            np.random.seed(random_state)
    
            data = np.random.binomial(1, sparsity, size=(num_points, n_features))
    
            actual_sparsity = np.mean(data)
    
            metadata = {
                'expected_sparsity': sparsity,
                'actual_sparsity': actual_sparsity
            }
    
            return t.tensor(data.astype(np.float32), device=self.device), metadata
        
        
    
        def multiple_gaussians(self, num_points=1000, n_dimensions=2, means=None, covariances=None, weights=None, random_state=None):
            """
            Generate a dataset of Gaussians with different densities.
    
            Parameters:
            - n_samples: int, total number of sample points
            - n_dimensions: int, number of dimensions for each point
            - means: list of arrays, each array is the mean of a Gaussian component
            - covariances: list of arrays, each array is the covariance matrix of a Gaussian component
            - weights: list of floats, relative weights (densities) of each Gaussian component
            - random_state: int, random seed for reproducibility
    
            Returns:
            - data: ndarray of shape (n_samples, n_dimensions)
            - metadata: dict containing component information and labels
            """
            np.random.seed(random_state)
            means = [
                [0, 0, 0],
                [3, 3, 3],
                [-2, 2, -2],
                [0, -4, 1]
            ]
            covariances = [
                [[1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Spherical
                [[2, 0, 0], [0, .5, 0], [0, 0, .5]],  # Elongated
                [[1, .8, .1], [.8, 1, .6], [.1, .6, 1]],  # Correlated
                [[.1, 0, 0], [0, .1, 0], [0, 0, .1]]  # Compact
            ]
            
            
            weights = [0.2, 0.2, 0.3, 0.3]  # Different densities
    
            """if means is None or covariances is None or weights is None:
                raise ValueError("means, covariances, and weights must be provided")
    
            if len(means) != len(covariances) or len(means) != len(weights):
                raise ValueError("Number of means, covariances, and weights must be the same")"""
    
            n_components = len(means)
    
            # Normalize weights
            weights = np.array(weights) / np.sum(weights)
    
            # Calculate number of samples for each component
            component_samples = np.random.multinomial(num_points, weights)
    
            data = []
            labels = []
    
            for i in range(n_components):
                if component_samples[i] > 0:
                    component_data = multivariate_normal.rvs(mean=means[i], cov=covariances[i], size=component_samples[i])
                    data.append(component_data)
                    labels.extend([i] * component_samples[i])
    
            data = np.vstack(data)
            labels = np.array(labels)
    
            # Shuffle the data and labels
            shuffle_idx = np.random.permutation(num_points)
            data = data[shuffle_idx]
            labels = labels[shuffle_idx]
    
            metadata = {
                'means': means,
                'covariances': covariances,
                'weights': weights,
                'labels': labels
            }
    
            return t.tensor(data.astype(np.float32), device=self.device), metadata
  </dt-code>
  


  </dt-appendix>

<script type="text/bibliography">
  @article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
 }

 @misc{rajamanoharan2024improving,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{Conmy2023,
  author = {Arthur Conmy},
  title = {My best guess at the important tricks for training 1L SAEs},
  year = {2023},
  url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
  note = {Accessed: 2024-05-30}
}

@misc{Marks2023,
  author = {Sam Marks},
  title = {Some open-source dictionaries and dictionary learning infrastructure},
  year = {2023},
  url = {https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning},
  note = {Accessed: 2024-05-30}
}

@misc{Nanda2023,
  author = {Neel Nanda},
  title = {Open Source Replication and Commentary on Anthropic's Dictionary Learning Paper},
  year = {2023},
  url = {https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s},
  note = {Accessed: 2024-05-30}
}

@misc{Anthropic2023,
  author = {Anthropic},
  title = {Advice for Training Sparse Autoencoders},
  year = {2023},
  url = {https://transformer-circuits.pub/2023/monosemantic-features#appendix-autoencoder},
  note = {Accessed: 2024-05-30}
}


@misc{McDougall2023,
  author = {Callum McDougall},
  title = {Intro to Superposition & Sparse Autoencoders (Colab exercises)},
  year = {2023},
  url = {https://www.lesswrong.com/posts/LnHowHgmrMbWtpkxx/intro-to-superposition-and-sparse-autoencoders-colab },
  note = {Accessed: 2024-05-30}
}

@misc{gao2024scalingsae,
  title={Scaling and evaluating sparse autoencoders}, 
  author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  year={2024},
  eprint={2406.04093},
  archivePrefix={arXiv},
  primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{rajamanoharan2024gatedsae,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{bricken2023monosemanticity,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
  year={2023},
  journal={Transformer Circuits Thread},
  note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@misc{Wright2024shrinkage,
  title={Addressing Feature Suppression in SAEs}, 
  author={Benjamin Wright and Lee Sharkey},
  year={2024},
  url = {https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes},
  note = {Accessed: 2024-05-30}
}

@article{Elhage2022superposition,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year={2022},
  journal={Transformer Circuits Thread},
  note={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@misc{Sharkey2024superposition,
  title={[Interim research report] Taking features out of superposition with sparse autoencoders}, 
  author={Lee Sharkey, Dan Braun and Beren Millidge},
  year={2024},
  url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition#Toy_dataset_generation},
  note = {Accessed: 2024-05-30}
}

//Linear representation hypothesis
@misc{park2024linearrepresentationhypothesisgeometry,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models}, 
  author={Kiho Park and Yo Joong Choe and Victor Veitch},
  year={2024},
  eprint={2311.03658},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2311.03658}, 
}

@misc{marks2023geometrytruthemergentlinear,
  title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets}, 
  author={Samuel Marks and Max Tegmark},
  year={2023},
  eprint={2310.06824},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2310.06824}, 
}

@misc{rajendran2024learninginterpretableconceptsunifying,
  title={Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models}, 
  author={Goutham Rajendran and Simon Buchholz and Bryon Aragam and Bernhard Schölkopf and Pradeep Ravikumar},
  year={2024},
  eprint={2402.09236},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2402.09236}, 
}

@misc{nanda2023emergentlinearrepresentationsworld,
  title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
  author={Neel Nanda and Andrew Lee and Martin Wattenberg},
  year={2023},
  eprint={2309.00941},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2309.00941}, 
}

@misc{tigges2023linearrepresentationssentimentlarge,
  title={Linear Representations of Sentiment in Large Language Models}, 
  author={Curt Tigges and Oskar John Hollinsworth and Atticus Geiger and Neel Nanda},
  year={2023},
  eprint={2310.15154},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2310.15154}, 
}

@misc{jiang2024originslinearrepresentationslarge,
  title={On the Origins of Linear Representations in Large Language Models}, 
  author={Yibo Jiang and Goutham Rajendran and Pradeep Ravikumar and Bryon Aragam and Victor Veitch},
  year={2024},
  eprint={2403.03867},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2403.03867}, 
}
@article{elhage2022superposition,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year={2022},
  journal={Transformer Circuits Thread},
  note={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{goh2016thoughtvectors,
  author = {Gabriel Goh},
  title = {Decoding the Thought Vector},
  journal = {Distill},
  year = {2016},
  note = {https://gabgoh.github.io/ThoughtVectors/},
}

@misc{burns2024discoveringlatentknowledgelanguage,
  title={Discovering Latent Knowledge in Language Models Without Supervision}, 
  author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
  year={2024},
  eprint={2212.03827},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2212.03827}, 
}

@misc{li2024inferencetimeinterventionelicitingtruthful,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model}, 
  author={Kenneth Li and Oam Patel and Fernanda Viégas and Hanspeter Pfister and Martin Wattenberg},
  year={2024},
  eprint={2306.03341},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2306.03341}, 
}

@misc{wang2024conceptalgebrascorebasedtextcontrolled,
  title={Concept Algebra for (Score-Based) Text-Controlled Generative Models}, 
  author={Zihao Wang and Lin Gui and Jeffrey Negrea and Victor Veitch},
  year={2024},
  eprint={2302.03693},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2302.03693}, 
}

@misc{trager2024linearspacesmeaningscompositional,
  title={Linear Spaces of Meanings: Compositional Structures in Vision-Language Models}, 
  author={Matthew Trager and Pramuditha Perera and Luca Zancato and Alessandro Achille and Parminder Bhatia and Stefano Soatto},
  year={2024},
  eprint={2302.14383},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2302.14383}, 
}
@article{olah2018buildingblocks,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@misc{bolukbasi2021interpretabilityillusionbert,
  title={An Interpretability Illusion for BERT}, 
  author={Tolga Bolukbasi and Adam Pearce and Ann Yuan and Andy Coenen and Emily Reif and Fernanda Viégas and Martin Wattenberg},
  year={2021},
  eprint={2104.07143},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2104.07143}, 
}
</script>

