<!doctype html>
<meta charset="utf-8">
<head>
    <style>
        button {
          background-color: #ffffff;
          color: rgb(229, 229, 229);
          border: 1px solid black;
          padding: 10px 20px;
          cursor: pointer;
          border-radius: 5px;
          margin-top: 20px;
          margin-left: 20px;
          font-size: 20px;
        }
      
        button:hover {
          color: #818181;
        }
        .active {
          color: #000000;
        }
      </style>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js" ></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="../distill.template.v1.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>


  <script src="train1.js" defer></script>

</head>

<script type="text/front-matter">
  title: "Toy SAE"
  description: "An exploration of sparse autoencoders trained on synthetic data"
  authors:
  - TBD: https://shivam-raval96.github.io/
  affiliations:
  - Harvard University
</script>


<dt-article >
  <div class="l-page">
  <h1>An exploration of sparse autoencoders trained on synthetic data</h1>
  <h2>[Under construction]</h2>
  <dt-byline></dt-byline>
  <p>Anthropic interpretability team recent showed Saprse Autoencoders (SAEs) can extract interpretable 
    features from Claude 3 Sonnet<dt-cite key="templeton2024scaling"></dt-cite>. We would like to understand
    s features on some synthetic datasets.
  </p>

  

  <h2>Sparse Autoencoders as feature extractors</h2>
  Autoencoders (AE) generally consist of an encoder and a decoder, 
  where the encoder maps the input to a latent space and the decoder reconstructs the input from this 
  latent space. One of the key objectives of an autoencoder is to capture the most important features 
  of the data necessary to reconstruct it. A sparse autoencoder (SAE) adds a sparsity penalty (L1 loss) to the loss function to learn sparse representations. For our purposes, we want to decompose an input \( x \in \mathbb{R}^d \) into a sparse, linear combination of feature directions:
  \[
  x = x_0 + \sum_{i=1}^M f(x)_i d_i,
  \]
  where \( d_i \in \mathbb{R}^d \) are \( M \gg d \) are the feature directions, and the sparse coefficients \( f(x) \geq 0 \) are
   the corresponding feature activations for the directions. Since it is possible to arbitrarily reduce the sparsity loss term without affecting reconstructions, we might have to constrain the norms of the columns of \( W_{\text{dec}} \)
  during training, as suggested by previous work from DeepMind<dt-cite key="rajamanoharan2024improving"></dt-cite>. Thus, this will be our model: for some input \(\mathbf{x} \in \mathbb{R}^d\), the encoder is:
  \[
  \mathbf{h} = \text{ReLU}(W_{\text{enc}} (x - x_{\text{dec}}) + b_{\text{enc}})
  \]
  The decoder reconstructs the input and can be represented as:
  \[
  \mathbf{\hat{x}} =  W_{\text{dec}} \mathbf{h} + b_{\text{dec}}
  \]

  The loss is the sum of the error in reconstruction of the input (typically the MSE) and a sparsity penalty on L1 penalty on the activations \(\mathbf{h} \in \mathbb{R}^M\)
  \[
L = L_{\text{reconstruction}} + \lambda L_{\text{sparsity}} = \|\mathbf{x} - \mathbf{\hat{x}}\|^2_2 + \lambda \sum_{i=1}^M |h_i|
\]  
</div>


<h2>Training an SAE on two dimensional gaussain data</h2>
<div class="l-page side"> 
  <div class="slider-container">
    <br/>
    <label for="d-slider">Hidden dimension:</label>
    <input type="range" id="d-slider" min="0" max="13" step="1"  value="2" oninput=updateImage("gaussian2d")>
    <span id="d-value">2</span>
    <br/>
    <label for="l-slider">lambda:</label>
    <input type="range" id="l-slider" min="0" max="12" step="1" value="0" oninput=updateImage("gaussian2d")>
    <span id="l-value">1e-2</span>
  </div>
  <div class="image-container">
    <img id="display-img" src="img/gaussian2d/10_0.005.png" width="100%" alt="[Move the slider if loading fails]">
  </div>
</div>
<div class="side">
  Let's train a SAE to reconstruct a Gaussian distribution \(\mathcal{N}(\mu = 0, \sigma = 1) \text{ in }\) in two dimensions. 
  The figures on the side show the original data (in blue) and the reconstructed data (in red) from a trained model for a range of hidden dimensions and the sparsity regularization parameter \( \lambda \)
  A datapoint and its reconstruction are connected by a thin grey line, as a visual aid.
  The lines in green are the learned feature directions. They are scaled by the activation value averaged across all inputs, i.e., \(\sum_{i=1}^M h_i\). If a feature direction is very small or near zero, its feature direction will not be prominent on the plot, and that feature is a "dead feature". It does not help in reconstruction
  <br/>
  <h4>Effect of sparsity regularization </h4>  High values enforces a stronger sparsity constraint on the hidden layer activations, encouraging most neurons to be inactive for any given input. As a result, the feature directions may become more distinct, but also result in many dead features. However, if \( \lambda \) is too high, it may hinder the SAE's ability to reconstruct the data accurately (not shown here).
 </div>

<script src="slider.js"></script>

<h2>Reconstructing data with high dimensionality</h2>
<div class="l-page button-bar"> 
<button class="button active" title="Data sampled from a unit normal distirbuion in 20D"  data="gaussian20d" onclick=updateData()>Gaussian20D</button>
<button class="button" data="3gaussian20d" onclick="updateData()">3 Gaussians in 20D </button>
<button class="button" data="5spokes20d" onclick="updateData()">5 Spokes in 20D </button>
<button class="button" data="sphere20d" onclick="updateData()">Spherical shell 20D </button>
<button class="button" data="circle2d" onclick="updateData()">Circle in 2D </button>
<button class="button" data="cube10d" onclick="updateData()">Cube edges in 10D </button>


</div>
<div class="slider-container">
  <br/>
  Hover on a point to view the hidden layer activations
  <br/>
  <label for="d-slider0">Hidden dimension:</label>
  <input type="range" id="d-slider0" min="0" max="8" step="1"  value="0" >
  <span id="d-value0">2</span>
  <br/>
  <label for="l-slider0">lambda:</label>
  <input type="range" id="l-slider0" min="0" max="12" step="1" value="0">
  <span id="l-value0">1</span>
</div>

<div style="display: flex;">
  <div id="scatterplot"></div>
  <div id="heatmap" class="'heatmap">
    
  </div>
</div>
<script src="makeplots.js"></script>



</div>
</div>

</dt-article>

<dt-appendix>
  <h4>Notes on training the SAE (the formatting may look weird depening on the screen resolution)</h4>
  <b>Reproducibility:</b>
  <dt-code block language="python">
    np.random.seed(42)
    torch.manual_seed(42)
  </dt-code>
  <b>Model definition:</b>
  <dt-code block language="python">
    class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(SparseAutoencoder, self).__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)  
        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)  
        self.b_dec = nn.Parameter(torch.zeros(input_dim))  
        self.b_enc = nn.Parameter(torch.zeros(hidden_dim))  
    
    def forward(self, x):
        # Encoder
        x_centered = x - self.b_dec 
        encoded = F.relu(self.encoder(x_centered) + self.b_enc)  
        
        # Decoder
        decoded = self.decoder(encoded) + self.b_dec  
        return decoded, encoded
  </dt-code>

  <b>Simplified training function. Note:</b>
  <p>1. All models were trained for 1000 epochs with Adam optimizer with lr = 0.001, with early stopping if the loss does not reduce for 200 epochs.</p>
  <p>2. After each optimizer step, decoder weights are normalized to have unit norm.</p>

  <dt-code block language="python">
  def train_sparse_autoencoder(model, dataloader, num_epochs, 
  lambda_l1, learning_rate=0.001, early_stop = True):
  
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    patience = 200  
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        for data in dataloader:
            inputs = data[0]
            
            outputs, encoded = model(inputs)
            mse_loss = criterion(outputs, inputs)
            
            l1_penalty = lambda_l1 * torch.sum(torch.abs(encoded))
            loss = mse_loss + l1_penalty
            epoch_loss += loss.item()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Normalize decoder weights to have unit norm
            with torch.no_grad():
                decoder_weights = model.decoder.weight.data
                norms = torch.norm(decoder_weights, dim=1, keepdim=True)
                model.decoder.weight.data = decoder_weights / norms
                
        epoch_loss /= len(dataloader)
        if early_stop:
            # Early stopping
            if epoch_loss < best_loss:
                best_loss = epoch_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f'Early stopping at epoch {epoch + 1}, Loss: {epoch_loss:.4f}')
                break

        
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')
  </dt-code>


  <b>Extract and plot feature directions, original and reconstructed data:</b>
  <dt-code block language="python">
  def extract_feature_directions(model):
      with torch.no_grad():
          decoder_weights = model.decoder.weight.data
          encoder_weights = model.encoder.weight.data

          # Each row of the weight matrix is a feature direction
          feature_directions = decoder_weights.clone()
          activations = encoder_weights.mean(axis=1).clone().cpu().numpy()

      return feature_directions.T, activations

  def plot_feature_directions(data, reconstructed, feature_directions, activations, title):
      range_x = [1.1 * data[:, 0].min(), 1.1 * data[:, 0].max()]
      range_y = [1.1 * data[:, 1].min(), 1.1 * data[:, 1].max()]

      fig = plt.figure(figsize=(6, 6))
      ax = fig.add_subplot()

      origin = np.array([0, 0, 0])
      ax.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10, label='Original data')
      ax.scatter(reconstructed[:, 0], reconstructed[:, 1], alpha=0.5, color='r', s=10, label='Reconstructed data')

      for i in range(len(activations)):
          ax.plot(
              [origin[0], feature_directions[i, 0] * activations[i]],
              [origin[1], feature_directions[i, 1] * activations[i]],
              color='limegreen', linewidth=3
          )
          ax.plot(
              [origin[0], -feature_directions[i, 0] * activations[i]],
              [origin[1], -feature_directions[i, 1] * activations[i]],
              color='limegreen', linewidth=3
          )

      feature_direction_proxy = plt.Line2D([0], [0], color='limegreen', linewidth=3)
      handles, labels = ax.get_legend_handles_labels()
      handles.append(feature_direction_proxy)
      labels.append('Feature directions')
      
      ax.legend(handles=handles, labels=labels, loc="upper right")
      plt.xticks([])
      plt.yticks([])
      plt.gca().set_aspect('equal')
      plt.xlim(range_x)
      plt.ylim(range_y)
      plt.savefig(title)
      plt.show()
  </dt-code>

  <b>Generate "Spokes" dataset:</b>
  <dt-code block language="python">
      def generate_spokes(num_vectors=3, num_points_per_vector=100, dim=3, scale_range=(0, 1)):

      # Generate random vectors in n dimensions
      vectors = np.random.randn(num_vectors, dim)
      
      # Initialize the list to collect points
      points = []
      
      for v in vectors:
          # Generate random scaling factors for the points along this vector
          scales = 2*np.random.random( num_points_per_vector)
          # Generate points along the vector
          for a in scales:
              points.append(a * v)
      
      # Convert the list of points to a numpy array
      points_array = np.array(points)
      
      return points_array.astype(np.float32)

  # Example usage
  data = generate_spokes(num_vectors=5, num_points_per_vector=200, dim=20, scale_range=(0, 5))
  </dt-code>

  <b>Generate multiple gaussians dataset:</b>
  <dt-code block language="python">
      def sample_from_mixture_with_labels(n_points, means, covariances, weights):
      
      n_gaussians = len(means)  # The number of Gaussians in the mixture
      d = means[0].shape[0]  # The dimensionality of the space
      
      # Validate inputs
      assert len(covariances) == n_gaussians, "Each Gaussian must have a covariance matrix."
      assert len(weights) == n_gaussians, "Each Gaussian must have a mixing weight."
      assert np.isclose(sum(weights), 1), "The mixing weights must sum to 1."
      
      # Choose which Gaussian each point will come from
      choices = np.random.choice(n_gaussians, size=n_points, p=weights)
      
      # Initialize an array to hold the generated samples
      samples = np.zeros((n_points, d))
      labels = np.zeros(n_points, dtype=int)
      
      # Generate samples from the chosen Gaussians
      for i in range(n_gaussians):
          # Number of points to sample from this Gaussian
          n_samples = np.sum(choices == i)
          
          if n_samples > 0:
              samples[choices == i] = np.random.multivariate_normal(
                  mean=means[i],
                  cov=covariances[i],
                  size=n_samples
              )
              labels[choices == i] = i
      
      return samples.astype(np.float32), labels


  #Example usage:
  n_points = 1000  # Number of points to generate
  dim = 20  # Dimensionality of the space

  means = [np.full(dim, -2), np.full(dim, 2),np.full(dim, 0)]  
  covariances = [np.eye(dim) * 1,  np.eye(dim) * 1,np.eye(dim) * 1] 
  weights = [0.33,0.33,0.34] 
  data, labels = sample_from_mixture_with_labels(n_points, means, covariances, weights)
  </dt-code>
  


  </dt-appendix>

<script type="text/bibliography">
  @article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
 }

 @misc{rajamanoharan2024improving,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
  author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
  year={2024},
  eprint={2404.16014},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
</script>

